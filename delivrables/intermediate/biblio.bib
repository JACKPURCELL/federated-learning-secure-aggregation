
@misc{DanielENSELMEDeveloppementpourmobiles2013,
  title = {Développement Pour Mobiles Avec {{Android}}},
  url = {https://www-techniques-ingenieur-fr.docelec.insa-lyon.fr/base-documentaire/technologies-de-l-information-th9/systemes-embarques-42588210/developpement-pour-mobiles-avec-android-h1590/},
  date = {2013-02-10},
  author = {Daniel ENSELME}
}

@incollection{Boudjeloud-AssalaApprentissagefederatifpour2019,
  langid = {french},
  title = {Apprentissage fédératif pour la prédiction du churn},
  isbn = {979-10-96289-09-7},
  abstract = {La sélection d'articles publiés dans le présent recueil constitue les actes de la 19e édition de la conférence francophone Extraction et Gestion des Connaissances (EGC 2019) qui s'est déroulée à Metz du 21 au 25 janvier 2019 sur le Campus de CentraleSupélec. L'objectif de ces journées scientifique est de rassembler des chercheurs de disciplines connexes (Bases de Données, Statistiques, Apprentissage, Représentation des Connaissances, Gestion des Connaissances et Fouille de Données) et les ingénieurs qui mettent en oeuvre sur des données réelles des méthodes d'extraction et de gestion des connaissances. Cette conférence est un évènement majeur fédérateur de la communauté francophone en Extraction et Gestion des Connaissances et regroupe des chercheurs de plusieurs pays (notamment France, Belgique, Luxembourg, Canada, Afrique du Nord). Le programme de la conférence comprend aussi des présentations de chercheurs invités reconnus mondialement pour leurs travaux. Les communications rassemblées dans ce volume traduisent à la fois le caractère multidisciplinaire des travaux de recherche présentés, la richesse des applications sous-jacentes et la vitalité des innovations issues de l'extraction et de la gestion des connaissances.},
  booktitle = {Extraction et Gestion des Connaissances: Actes de la conférence EGC'2019},
  publisher = {{BoD - Books on Demand}},
  date = {2019-01-08},
  pages = {141},
  keywords = {Art / General,Computers / Information Technology},
  author = {Boudjeloud-Assala, Lydia},
  eprinttype = {googlebooks},
  eprint = {MveCDwAAQBAJ}
}

@video{YufengGuoOndevicemachinelearning,
  title = {On-Device Machine Learning: {{TensorFlow}} on {{Android}} ({{Google Cloud Next}} '17)},
  url = {https://www.youtube.com/watch?v=EnFyneRScQ8},
  shorttitle = {On-Device Machine Learning},
  abstract = {In this video, Yufeng Guo applies deep learning models to local prediction on mobile devices. Yufeng shows you how to use TensorFlow to implement a machine learning model that is tailored to a custom dataset. You will come away knowing enough to get started solving your own deep learning problems.},
  urldate = {2019-10-09},
  keywords = {android,GCloudNext17,lite,tensorflow},
  editor = {Yufeng Guo},
  editortype = {director}
}

@video{3Blue1BrownMaisquestce,
  title = {Mais Qu'est-Ce Qu'un Réseau de Neurones ? | {{Chapitre}} 1},
  url = {https://www.youtube.com/watch?v=aircAruvnKk},
  shorttitle = {Mais Qu'est-Ce Qu'un Réseau de Neurones ?},
  urldate = {2019-10-09},
  editor = {3Blue1Brown},
  editortype = {director}
}

@video{3Blue1Browndescentegradientou,
  title = {La Descente de Gradient, Ou Comment Un Réseau Neuronal Apprend | {{Chapitre}} 2},
  url = {https://www.youtube.com/watch?v=IHZwWFHWa-w},
  urldate = {2019-10-09},
  editor = {3Blue1Brown},
  editortype = {director}
}

@video{3Blue1BrownQuefaitvraiment,
  title = {Que Fait Vraiment La Rétropropagation? | {{Chapitre}} 3},
  url = {https://www.youtube.com/watch?v=Ilg3gGewQ5U},
  shorttitle = {What Is Backpropagation Really Doing?},
  abstract = {What's actually happening to a neural network as it learns?

The following video is sort of an appendix to this one.  The main goal with the follow-on video is to show the connection between the visual walkthrough here, and the representation of these "nudges" in terms of partial derivatives that you will find when reading about backpropagation in other resources, like Michael Nielsen's book or Chis Olah's blog.},
  urldate = {2019-10-09},
  editor = {3Blue1Brown},
  editortype = {director}
}

@software{MatthijsHollemansRepositoryhollancecoremltraining2019,
  title = {Repository Hollance/Coreml-Training},
  url = {https://github.com/hollance/coreml-training},
  abstract = {Source code for my blog post series "On-device training with Core ML". Training a model on Apple devices using Apple CoreML librairy.},
  urldate = {2019-10-15},
  date = {2019-10-15T06:07:52Z},
  keywords = {coreml,edge-computing,ios,machine-learning,on-device-ai,swift},
  author = {Matthijs Hollemans},
  origdate = {2019-07-19T09:05:24Z}
}

@online{MatthijsHollemansTrainingIOSdevice,
  title = {Training on {{IOS}} Device},
  url = {https://machinethink.net/blog/training-on-device/},
  abstract = {State of the art concerning training on Apple IOS device.},
  urldate = {2019-10-15},
  author = {Matthijs Hollemans},
  file = {/home/corentin/Zotero/storage/IWUGKYKN/training-on-device.html}
}

@online{FranksBuildingCustomMachine2018,
  langid = {english},
  title = {Building a {{Custom Machine Learning Model}} on {{Android}} with {{TensorFlow Lite}}},
  url = {https://medium.com/over-engineering/building-a-custom-machine-learning-model-on-android-with-tensorflow-lite-26447e53abf2},
  abstract = {Building a custom TensorFlow Lite model sounds really scary. As it turns out, you don’t need to be a Machine Learning or TensorFlow expert…},
  journaltitle = {Medium},
  urldate = {2019-10-16},
  date = {2018-08-07T09:56:01.078Z},
  author = {Franks, Rebecca},
  file = {/home/corentin/Zotero/storage/VDFD2D8M/building-a-custom-machine-learning-model-on-android-with-tensorflow-lite-26447e53abf2.html}
}

@article{GaberDatastreammining2014,
  langid = {english},
  title = {Data Stream Mining in Ubiquitous Environments: State-of-the-Art and Current Directions},
  volume = {4},
  issn = {1942-4795},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1115},
  doi = {10.1002/widm.1115},
  shorttitle = {Data Stream Mining in Ubiquitous Environments},
  abstract = {In this article, we review the state-of-the-art techniques in mining data streams for mobile and ubiquitous environments. We start the review with a concise background of data stream processing, presenting the building blocks for mining data streams. In a wide range of applications, data streams are required to be processed on small ubiquitous devices like smartphones and sensor devices. Mobile and ubiquitous data mining target these applications with tailored techniques and approaches addressing scarcity of resources and mobility issues. Two categories can be identified for mobile and ubiquitous mining of streaming data: single-node and distributed. This survey will cover both categories. Mining mobile and ubiquitous data require algorithms with the ability to monitor and adapt the working conditions to the available computational resources. We identify the key characteristics of these algorithms and present illustrative applications. Distributed data stream mining in the mobile environment is then discussed, presenting the Pocket Data Mining framework. Mobility of users stimulates the adoption of context-awareness in this area of research. Context-awareness and collaboration are discussed in the Collaborative Data Stream Mining, where agents share knowledge to learn adaptive accurate models. This article is categorized under: Fundamental Concepts of Data and Knowledge {$>$} Key Design Issues in Data Mining Fundamental Concepts of Data and Knowledge {$>$} Motivation and Emergence of Data Mining Technologies {$>$} Structure Discovery and Clustering},
  number = {2},
  journaltitle = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  urldate = {2019-10-20},
  date = {2014},
  pages = {116-138},
  author = {Gaber, Mohamed Medhat and Gama, João and Krishnaswamy, Shonali and Gomes, João Bártolo and Stahl, Frederic},
  file = {/home/corentin/Zotero/storage/E6VUG5R2/Gaber et al. - 2014 - Data stream mining in ubiquitous environments sta.pdf;/home/corentin/Zotero/storage/MIR8V3QU/widm.html}
}

@inproceedings{BrezmesActivityRecognitionAccelerometer2009,
  langid = {english},
  location = {{Berlin, Heidelberg}},
  title = {Activity {{Recognition}} from {{Accelerometer Data}} on a {{Mobile Phone}}},
  isbn = {978-3-642-02481-8},
  doi = {10.1007/978-3-642-02481-8_120},
  abstract = {Real-time monitoring of human movements can be easily envisaged as a useful tool for many purposes and future applications. This paper presents the implementation of a real-time classification system for some basic human movements using a conventional mobile phone equipped with an accelerometer. The aim of this study was to check the present capacity of conventional mobile phones to execute in real-time all the necessary pattern recognition algorithms to classify the corresponding human movements. No server processing data is involved in this approach, so the human monitoring is completely decentralized and only an additional software will be required to remotely report the human monitoring. The feasibility of this approach opens a new range of opportunities to develop new applications at a reasonable low-cost.},
  booktitle = {Distributed {{Computing}}, {{Artificial Intelligence}}, {{Bioinformatics}}, {{Soft Computing}}, and {{Ambient Assisted Living}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer}},
  date = {2009},
  pages = {796-799},
  keywords = {accelerometer,human movement’s detection,Pattern recognition},
  author = {Brezmes, Tomas and Gorricho, Juan-Luis and Cotrina, Josep},
  editor = {Omatu, Sigeru and Rocha, Miguel P. and Bravo, José and Fernández, Florentino and Corchado, Emilio and Bustillo, Andrés and Corchado, Juan M.},
  file = {/home/corentin/Zotero/storage/7XVKLBER/Brezmes et al. - 2009 - Activity Recognition from Accelerometer Data on a .pdf}
}

@inproceedings{SunActivityRecognitionAccelerometer2010a,
  langid = {english},
  location = {{Berlin, Heidelberg}},
  title = {Activity {{Recognition}} on an {{Accelerometer Embedded Mobile Phone}} with {{Varying Positions}} and {{Orientations}}},
  isbn = {978-3-642-16355-5},
  doi = {10.1007/978-3-642-16355-5_42},
  abstract = {This paper uses accelerometer-embedded mobile phones to monitor one’s daily physical activities for sake of changing people’s sedentary lifestyle. In contrast to the previous work of recognizing user’s physical activities by using a single accelerometer-embedded device and placing it in a known position or fixed orientation, this paper intends to recognize the physical activities in the natural setting where the mobile phone’s position and orientation are varying, depending on the position, material and size of the hosting pocket. By specifying 6 pocket positions, this paper develops a SVM based classifier to recognize 7 common physical activities. Based on 10-folder cross validation result on a 48.2 hour data set collected from 7 subjects, our solution outperforms Yang’s solution and SHPF solution by 5\textasciitilde{}6\%. By introducing an orientation insensitive sensor reading dimension, we boost the overall F-score from 91.5\% to 93.1\%. With known pocket position, the overall F-score increases to 94.8\%.},
  booktitle = {Ubiquitous {{Intelligence}} and {{Computing}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer}},
  date = {2010},
  pages = {548-562},
  keywords = {accelerometer,Activity recognition,mobile phone,SVM},
  author = {Sun, Lin and Zhang, Daqing and Li, Bin and Guo, Bin and Li, Shijian},
  editor = {Yu, Zhiwen and Liscano, Ramiro and Chen, Guanling and Zhang, Daqing and Zhou, Xingshe},
  file = {/home/corentin/Zotero/storage/H3GN9QNX/Sun et al. - 2010 - Activity Recognition on an Accelerometer Embedded .pdf}
}

@online{EclipsefoundationKerasImportOverview,
  title = {Keras {{Import Overview}} | {{Deeplearning4j}}},
  url = {https://deeplearning4j.org/docs/latest/keras-import-overview},
  abstract = {A part of the DL4J documentation focus on importing keras based model in DL4J librairy.},
  urldate = {2019-10-24},
  author = {Eclipse foundation},
  file = {/home/corentin/Zotero/storage/TYRCNZZ3/keras-import-overview.html}
}

@online{EclipsefoundationMaindocumentationDeeplearning4j,
  title = {Main Documentation | {{Deeplearning4j}}},
  url = {https://deeplearning4j.org/docs/latest/},
  abstract = {Index web page of the DL4J librairy documentation.},
  urldate = {2019-10-24},
  author = {Eclipse foundation},
  file = {/home/corentin/Zotero/storage/F7DR9ULK/latest.html}
}

@article{WangDeeplearningsensorbased2019,
  langid = {english},
  title = {Deep Learning for Sensor-Based Activity Recognition: {{A}} Survey},
  volume = {119},
  issn = {0167-8655},
  url = {http://www.sciencedirect.com/science/article/pii/S016786551830045X},
  doi = {10.1016/j.patrec.2018.02.010},
  shorttitle = {Deep Learning for Sensor-Based Activity Recognition},
  abstract = {Sensor-based activity recognition seeks the profound high-level knowledge about human activities from multitudes of low-level sensor readings. Conventional pattern recognition approaches have made tremendous progress in the past years. However, those methods often heavily rely on heuristic hand-crafted feature extraction, which could hinder their generalization performance. Additionally, existing methods are undermined for unsupervised and incremental learning tasks. Recently, the recent advancement of deep learning makes it possible to perform automatic high-level feature extraction thus achieves promising performance in many areas. Since then, deep learning based methods have been widely adopted for the sensor-based activity recognition tasks. This paper surveys the recent advance of deep learning based sensor-based activity recognition. We summarize existing literature from three aspects: sensor modality, deep model, and application. We also present detailed insights on existing work and propose grand challenges for future research.},
  series = {Deep {{Learning}} for {{Pattern Recognition}}},
  urldate = {2019-10-24},
  date = {2019-03-01},
  pages = {3-11},
  keywords = {Pattern recognition,Activity recognition,Deep learning,Pervasive computing},
  author = {Wang, Jindong and Chen, Yiqiang and Hao, Shuji and Peng, Xiaohui and Hu, Lisha},
  file = {/home/corentin/Zotero/storage/V39LQYTG/Wang et al. - 2019 - Deep learning for sensor-based activity recognitio.pdf;/home/corentin/Zotero/storage/4RSQFMQ6/S016786551830045X.html}
}

@article{MuradDeepRecurrentNeural2017,
  langid = {english},
  title = {Deep {{Recurrent Neural Networks}} for {{Human Activity Recognition}}},
  volume = {17},
  url = {https://www.mdpi.com/1424-8220/17/11/2556},
  doi = {10.3390/s17112556},
  abstract = {Adopting deep learning methods for human activity recognition has been effective in extracting discriminative features from raw input sequences acquired from body-worn sensors. Although human movements are encoded in a sequence of successive samples in time, typical machine learning methods perform recognition tasks without exploiting the temporal correlations between input data samples. Convolutional neural networks (CNNs) address this issue by using convolutions across a one-dimensional temporal sequence to capture dependencies among input data. However, the size of convolutional kernels restricts the captured range of dependencies between data samples. As a result, typical models are unadaptable to a wide range of activity-recognition configurations and require fixed-length input windows. In this paper, we propose the use of deep recurrent neural networks (DRNNs) for building recognition models that are capable of capturing long-range dependencies in variable-length input sequences. We present unidirectional, bidirectional, and cascaded architectures based on long short-term memory (LSTM) DRNNs and evaluate their effectiveness on miscellaneous benchmark datasets. Experimental results show that our proposed models outperform methods employing conventional machine learning, such as support vector machine (SVM) and k-nearest neighbors (KNN). Additionally, the proposed models yield better performance than other deep learning techniques, such as deep believe networks (DBNs) and CNNs.},
  number = {11},
  journaltitle = {Sensors},
  urldate = {2019-10-24},
  date = {2017-11},
  pages = {2556},
  keywords = {deep learning,human activity recognition,recurrent neural networks},
  author = {Murad, Abdulmajid and Pyun, Jae-Young},
  file = {/home/corentin/Zotero/storage/SUD64XWH/Murad and Pyun - 2017 - Deep Recurrent Neural Networks for Human Activity .pdf;/home/corentin/Zotero/storage/GD9VWWID/2556.html}
}

@article{OrdonezDeepConvolutionalLSTM2016,
  langid = {english},
  title = {Deep {{Convolutional}} and {{LSTM Recurrent Neural Networks}} for {{Multimodal Wearable Activity Recognition}}},
  volume = {16},
  url = {https://www.mdpi.com/1424-8220/16/1/115},
  doi = {10.3390/s16010115},
  abstract = {Human activity recognition (HAR) tasks have traditionally been solved using engineered features obtained by heuristic processes. Current research suggests that deep convolutional neural networks are suited to automate feature extraction from raw sensor inputs. However, human activities are made of complex sequences of motor movements, and capturing this temporal dynamics is fundamental for successful HAR. Based on the recent success of recurrent neural networks for time series domains, we propose a generic deep framework for activity recognition based on convolutional and LSTM recurrent units, which: (i) is suitable for multimodal wearable sensors; (ii) can perform sensor fusion naturally; (iii) does not require expert knowledge in designing features; and (iv) explicitly models the temporal dynamics of feature activations. We evaluate our framework on two datasets, one of which has been used in a public activity recognition challenge. Our results show that our framework outperforms competing deep non-recurrent networks on the challenge dataset by 4\% on average; outperforming some of the previous reported results by up to 9\%. Our results show that the framework can be applied to homogeneous sensor modalities, but can also fuse multimodal sensors to improve performance. We characterise key architectural hyperparameters’ influence on performance to provide insights about their optimisation.},
  number = {1},
  journaltitle = {Sensors},
  urldate = {2019-10-24},
  date = {2016-01},
  pages = {115},
  keywords = {deep learning,human activity recognition,LSTM,machine learning,neural network,sensor fusion,wearable sensors},
  author = {Ordóñez, Francisco Javier and Roggen, Daniel},
  file = {/home/corentin/Zotero/storage/HLP8TFDW/Ordóñez and Roggen - 2016 - Deep Convolutional and LSTM Recurrent Neural Netwo.pdf;/home/corentin/Zotero/storage/XHHIFBSV/html.html}
}

@software{BhaskarUdiBhaskarHumanActivityRecognitionUsingDeepNN2019,
  title = {{{UdiBhaskar}}/{{Human}}-{{Activity}}-{{Recognition}}--{{Using}}-{{Deep}}-{{NN}}},
  url = {https://github.com/UdiBhaskar/Human-Activity-Recognition--Using-Deep-NN},
  abstract = {Human Activity Recognition Using Deep Learning. Contribute to UdiBhaskar/Human-Activity-Recognition--Using-Deep-NN development by creating an account on GitHub.},
  urldate = {2019-10-24},
  date = {2019-10-22T02:41:07Z},
  author = {Bhaskar, Uday},
  origdate = {2018-11-12T12:14:09Z}
}

@article{Bullingtutorialhumanactivity2014,
  langid = {english},
  title = {A Tutorial on Human Activity Recognition Using Body-Worn Inertial Sensors},
  volume = {46},
  issn = {03600300},
  url = {http://dl.acm.org/citation.cfm?doid=2578702.2499621},
  doi = {10.1145/2499621},
  number = {3},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  urldate = {2019-10-24},
  date = {2014-01-01},
  pages = {1-33},
  author = {Bulling, Andreas and Blanke, Ulf and Schiele, Bernt},
  file = {/home/corentin/Zotero/storage/RLRGTIL3/Bulling et al. - 2014 - A tutorial on human activity recognition using bod.pdf}
}

@article{VoicuHumanPhysicalActivity2019,
  title = {Human {{Physical Activity Recognition Using Smartphone Sensors}}},
  volume = {19},
  issn = {1424-8220},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6386882/},
  doi = {10.3390/s19030458},
  abstract = {Because the number of elderly people is predicted to increase quickly in the upcoming years, “aging in place” (which refers to living at home regardless of age and other factors) is becoming an important topic in the area of ambient assisted living. Therefore, in this paper, we propose a human physical activity recognition system based on data collected from smartphone sensors. The proposed approach implies developing a classifier using three sensors available on a smartphone: accelerometer, gyroscope, and gravity sensor. We have chosen to implement our solution on mobile phones because they are ubiquitous and do not require the subjects to carry additional sensors that might impede their activities. For our proposal, we target walking, running, sitting, standing, ascending, and descending stairs. We evaluate the solution against two datasets (an internal one collected by us and an external one) with great effect. Results show good accuracy for recognizing all six activities, with especially good results obtained for walking, running, sitting, and standing. The system is fully implemented on a mobile device as an Android application.},
  number = {3},
  journaltitle = {Sensors (Basel, Switzerland)},
  shortjournal = {Sensors (Basel)},
  urldate = {2019-10-24},
  date = {2019-01-23},
  author = {Voicu, Robert-Andrei and Dobre, Ciprian and Bajenaru, Lidia and Ciobanu, Radu-Ioan},
  file = {/home/corentin/Zotero/storage/8CLRVUC7/Voicu et al. - 2019 - Human Physical Activity Recognition Using Smartpho.pdf},
  eprinttype = {pmid},
  eprint = {30678039},
  pmcid = {PMC6386882}
}

@incollection{SunActivityRecognitionAccelerometer2010,
  langid = {english},
  location = {{Berlin, Heidelberg}},
  title = {Activity {{Recognition}} on an {{Accelerometer Embedded Mobile Phone}} with {{Varying Positions}} and {{Orientations}}},
  volume = {6406},
  isbn = {978-3-642-16354-8 978-3-642-16355-5},
  url = {http://link.springer.com/10.1007/978-3-642-16355-5_42},
  doi = {10.1007/978-3-642-16355-5_42},
  abstract = {This paper uses accelerometer-embedded mobile phones to monitor one’s daily physical activities for sake of changing people’s sedentary lifestyle. In contrast to the previous work of recognizing user’s physical activities by using a single accelerometer-embedded device and placing it in a known position or fixed orientation, this paper intends to recognize the physical activities in the natural setting where the mobile phone’s position and orientation are varying, depending on the position, material and size of the hosting pocket. By specifying 6 pocket positions, this paper develops a SVM based classifier to recognize 7 common physical activities. Based on 10-folder cross validation result on a 48.2 hour data set collected from 7 subjects, our solution outperforms Yang’s solution and SHPF solution by 5∼6\%. By introducing an orientation insensitive sensor reading dimension, we boost the overall F-score from 91.5\% to 93.1\%. With known pocket position, the overall F-score increases to 94.8\%.},
  booktitle = {Ubiquitous {{Intelligence}} and {{Computing}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2019-10-24},
  date = {2010},
  pages = {548-562},
  author = {Sun, Lin and Zhang, Daqing and Li, Bin and Guo, Bin and Li, Shijian},
  editor = {Yu, Zhiwen and Liscano, Ramiro and Chen, Guanling and Zhang, Daqing and Zhou, Xingshe}
}

@article{BonawitzFederatedLearningScale2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.01046},
  primaryClass = {cs, stat},
  title = {Towards {{Federated Learning}} at {{Scale}}: {{System Design}}},
  url = {http://arxiv.org/abs/1902.01046},
  shorttitle = {Towards {{Federated Learning}} at {{Scale}}},
  abstract = {Federated Learning is a distributed machine learning approach which enables model training on a large corpus of decentralized data. We have built a scalable production system for Federated Learning in the domain of mobile devices, based on TensorFlow. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.},
  urldate = {2019-11-28},
  date = {2019-03-22},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Bonawitz, Keith and Eichner, Hubert and Grieskamp, Wolfgang and Huba, Dzmitry and Ingerman, Alex and Ivanov, Vladimir and Kiddon, Chloe and Konečný, Jakub and Mazzocchi, Stefano and McMahan, H. Brendan and Van Overveldt, Timon and Petrou, David and Ramage, Daniel and Roselander, Jason},
  file = {/home/corentin/Zotero/storage/UGMIVE6T/Bonawitz et al. - 2019 - Towards Federated Learning at Scale System Design.pdf;/home/corentin/Zotero/storage/6E6XF5FI/1902.html}
}

@inproceedings{BonawitzPracticalSecureAggregation2017,
  langid = {english},
  location = {{Dallas, Texas, USA}},
  title = {Practical {{Secure Aggregation}} for {{Privacy}}-{{Preserving Machine Learning}}},
  isbn = {978-1-4503-4946-8},
  url = {http://dl.acm.org/citation.cfm?doid=3133956.3133982},
  doi = {10.1145/3133956.3133982},
  abstract = {We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user’s individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers 1.73× communication expansion for 210 users and 220-dimensional vectors, and 1.98× expansion for 214 users and 224-dimensional vectors over sending data in the clear.},
  eventtitle = {The 2017 {{ACM SIGSAC Conference}}},
  booktitle = {Proceedings of the 2017 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}} - {{CCS}} '17},
  publisher = {{ACM Press}},
  urldate = {2019-11-28},
  date = {2017},
  pages = {1175-1191},
  author = {Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H. Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  file = {/home/corentin/Zotero/storage/VGWJ97HC/Bonawitz et al. - 2017 - Practical Secure Aggregation for Privacy-Preservin.pdf}
}

@inproceedings{BonawitzPracticalSecureAggregation2017a,
  location = {{New York, NY, USA}},
  title = {Practical {{Secure Aggregation}} for {{Privacy}}-{{Preserving Machine Learning}}},
  isbn = {978-1-4503-4946-8},
  url = {http://doi.acm.org/10.1145/3133956.3133982},
  doi = {10.1145/3133956.3133982},
  abstract = {We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user's individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers \$1.73 x communication expansion for 210 users and 220-dimensional vectors, and 1.98 x expansion for 214 users and 224-dimensional vectors over sending data in the clear.},
  booktitle = {Proceedings of the 2017 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  series = {{{CCS}} '17},
  publisher = {{ACM}},
  urldate = {2019-11-28},
  date = {2017},
  pages = {1175--1191},
  keywords = {machine learning,federated learning,privacy-preserving protocols,secure aggregation},
  author = {Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H. Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  file = {/home/corentin/Zotero/storage/3D54XR6L/Bonawitz et al. - 2017 - Practical Secure Aggregation for Privacy-Preservin.pdf},
  venue = {Dallas, Texas, USA}
}

@article{ShamirHowShareSecret1979,
  title = {How to {{Share}} a {{Secret}}},
  volume = {22},
  issn = {0001-0782},
  url = {http://doi.acm.org/10.1145/359168.359176},
  doi = {10.1145/359168.359176},
  abstract = {In this paper we show how to divide data D into n pieces in such a way that D is easily reconstructable from any k pieces, but even complete knowledge of k - 1 pieces reveals absolutely no information about D. This technique enables the construction of robust key management schemes for cryptographic systems that can function securely and reliably even when misfortunes destroy half the pieces and security breaches expose all but one of the remaining pieces.},
  number = {11},
  journaltitle = {Commun. ACM},
  urldate = {2019-12-02},
  date = {1979-11},
  pages = {612--613},
  keywords = {cryptography,interpolation,key management},
  author = {Shamir, Adi}
}

@article{MalekzadehPrivacyUtilityPreserving2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1911.05996},
  primaryClass = {cs, eess, stat},
  title = {Privacy and {{Utility Preserving Sensor}}-{{Data Transformations}}},
  url = {http://arxiv.org/abs/1911.05996},
  abstract = {Sensitive inferences and user re-identification are major threats to privacy when raw sensor data from wearable or portable devices are shared with cloud-assisted applications. To mitigate these threats, we propose mechanisms to transform sensor data before sharing them with applications running on users' devices. These transformations aim at eliminating patterns that can be used for user re-identification or for inferring potentially sensitive activities, while introducing a minor utility loss for the target application (or task). We show that, on gesture and activity recognition tasks, we can prevent inference of potentially sensitive activities while keeping the reduction in recognition accuracy of non-sensitive activities to less than 5 percentage points. We also show that we can reduce the accuracy of user re-identification and of the potential inference of gender to the level of a random guess, while keeping the accuracy of activity recognition comparable to that obtained on the original data.},
  urldate = {2019-12-03},
  date = {2019-11-14},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  author = {Malekzadeh, Mohammad and Clegg, Richard G. and Cavallaro, Andrea and Haddadi, Hamed},
  file = {/home/corentin/Zotero/storage/EHB7B835/Malekzadeh et al. - 2019 - Privacy and Utility Preserving Sensor-Data Transfo.pdf;/home/corentin/Zotero/storage/8UMBJRFY/1911.html},
  annotation = {Comment: Accepted to appear in Pervasive and Mobile computing (PMC) Journal, Elsevier}
}

@inproceedings{MalekzadehMobileSensorData2019,
  location = {{New York, NY, USA}},
  title = {Mobile {{Sensor Data Anonymization}}},
  isbn = {978-1-4503-6283-2},
  url = {http://doi.acm.org/10.1145/3302505.3310068},
  doi = {10.1145/3302505.3310068},
  abstract = {Motion sensors such as accelerometers and gyroscopes measure the instant acceleration and rotation of a device, in three dimensions. Raw data streams from motion sensors embedded in portable and wearable devices may reveal private information about users without their awareness. For example, motion data might disclose the weight or gender of a user, or enable their re-identification. To address this problem, we propose an on-device transformation of sensor data to be shared for specific applications, such as monitoring selected daily activities, without revealing information that enables user identification. We formulate the anonymization problem using an information-theoretic approach and propose a new multi-objective loss function for training deep autoencoders. This loss function helps minimizing user-identity information as well as data distortion to preserve the application-specific utility. The training process regulates the encoder to disregard user-identifiable patterns and tunes the decoder to shape the output independently of users in the training set. The trained autoencoder can be deployed on a mobile or wearable device to anonymize sensor data even for users who are not included in the training dataset. Data from 24 users transformed by the proposed anonymizing autoencoder lead to a promising trade-off between utility and privacy, with an accuracy for activity recognition above 92\% and an accuracy for user identification below 7\%.},
  booktitle = {Proceedings of the {{International Conference}} on {{Internet}} of {{Things Design}} and {{Implementation}}},
  series = {{{IoTDI}} '19},
  publisher = {{ACM}},
  urldate = {2019-12-03},
  date = {2019},
  pages = {49--58},
  keywords = {adversarial training,deep learning,edge computing,sensor data privacy,time series analysis},
  author = {Malekzadeh, Mohammad and Clegg, Richard G. and Cavallaro, Andrea and Haddadi, Hamed},
  file = {/home/corentin/Zotero/storage/IVVCHIJV/Malekzadeh et al. - 2019 - Mobile Sensor Data Anonymization.pdf},
  venue = {Montreal, Quebec, Canada}
}

@inproceedings{AnguitaPublicDomainDataset2013,
  title = {A {{Public Domain Dataset}} for {{Human Activity Recognition}} Using {{Smartphones}}},
  date = {2013-01-01},
  author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Parra, Xavier and Reyes-Ortiz, J}
}

@online{AndroidDeepLearning,
  title = {Android for {{Deep Learning}} | {{Deeplearning4j}}},
  url = {https://deeplearning4j.org/docs/latest/deeplearning4j-android},
  urldate = {2019-12-05},
  file = {/home/corentin/Zotero/storage/KQWEM6LI/deeplearning4j-android.html}
}

@incollection{BaoActivityRecognitionUserAnnotated2004,
  langid = {english},
  location = {{Berlin, Heidelberg}},
  title = {Activity {{Recognition}} from {{User}}-{{Annotated Acceleration Data}}},
  volume = {3001},
  isbn = {978-3-540-21835-7 978-3-540-24646-6},
  url = {http://link.springer.com/10.1007/978-3-540-24646-6_1},
  doi = {10.1007/978-3-540-24646-6_1},
  abstract = {In this work, algorithms are developed and evaluated to detect physical activities from data acquired using five small biaxial accelerometers worn simultaneously on different parts of the body. Acceleration data was collected from 20 subjects without researcher supervision or observation. Subjects were asked to perform a sequence of everyday tasks but not told specifically where or how to do them. Mean, energy, frequency-domain entropy, and correlation of acceleration data was calculated and several classifiers using these features were tested. Decision tree classifiers showed the best performance recognizing everyday activities with an overall accuracy rate of 84\%. The results show that although some activities are recognized well with subject-independent training data, others appear to require subject-specific training data. The results suggest that multiple accelerometers aid in recognition because conjunctions in acceleration feature values can effectively discriminate many activities. With just two biaxial accelerometers – thigh and wrist – the recognition performance dropped only slightly. This is the first work to investigate performance of recognition algorithms with multiple, wire-free accelerometers on 20 activities using datasets annotated by the subjects themselves.},
  booktitle = {Pervasive {{Computing}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2019-12-05},
  date = {2004},
  pages = {1-17},
  author = {Bao, Ling and Intille, Stephen S.},
  editor = {Ferscha, Alois and Mattern, Friedemann},
  editorb = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y.},
  editorbtype = {redactor},
  file = {/home/corentin/Zotero/storage/9ZVKQW9P/Bao et Intille - 2004 - Activity Recognition from User-Annotated Accelerat.pdf}
}

@online{KerasImportOverview,
  title = {Keras {{Import Overview}} | {{Deeplearning4j}}},
  url = {http://deeplearning4j.org/docs/latest/keras-import-overview},
  urldate = {2019-12-05},
  file = {/home/corentin/Zotero/storage/PSX57YI7/keras-import-overview.html}
}

@online{DivyanshuSharmaImplementingFederatedLearning,
  title = {Implementing {{Federated Learning}} in {{Android}}},
  url = {https://vision-air.github.io/federated.html},
  abstract = {Harshita Diddee},
  urldate = {2019-12-05},
  author = {Divyanshu Sharma and Shivani Jindal and Shivam Grover},
  file = {/home/corentin/Zotero/storage/TDZMBA5A/federated.html}
}

@online{EdT5IFADE,
  title = {{{EdT}} - {{5IF}}@{{ADE}}},
  url = {https://servif-cocktail.insa-lyon.fr/EdT/5IF.php},
  urldate = {2019-12-05},
  file = {/home/corentin/Zotero/storage/T7EJFI4Y/5IF.html}
}

@online{Motionsensors,
  langid = {english},
  title = {Motion Sensors},
  url = {https://developer.android.com/guide/topics/sensors/sensors_motion},
  journaltitle = {Android Developers},
  urldate = {2019-12-05},
  file = {/home/corentin/Zotero/storage/5TJEPKXH/sensors_motion.html}
}

@inproceedings{AlzantotRSTensorFlowGPUEnabled2017,
  langid = {english},
  location = {{Niagara Falls, New York, USA}},
  title = {{{RSTensorFlow}}: {{GPU Enabled TensorFlow}} for {{Deep Learning}} on {{Commodity Android Devices}}},
  isbn = {978-1-4503-4962-8},
  url = {http://dl.acm.org/citation.cfm?doid=3089801.3089805},
  doi = {10.1145/3089801.3089805},
  shorttitle = {{{RSTensorFlow}}},
  abstract = {Mobile devices have become an essential part of our daily lives. By virtue of both their increasing computing power and the recent progress made in AI, mobile devices evolved to act as intelligent assistants in many tasks rather than a mere way of making phone calls. However, popular and commonly used tools and frameworks for machine intelligence are still lacking the ability to make proper use of the available heterogeneous computing resources on mobile devices. In this paper, we study the benefits of utilizing the heterogeneous (CPU and GPU) computing resources available on commodity android devices while running deep learning models. We leveraged the heterogeneous computing framework RenderScript to accelerate the execution of deep learning models on commodity Android devices. Our system is implemented as an extension to the popular opensource framework TensorFlow. By integrating our acceleration framework tightly into TensorFlow, machine learning engineers can now easily make benefit of the heterogeneous computing resources on mobile devices without the need of any extra tools. We evaluate our system on different android phones models to study the trade-offs of running different neural network operations on the GPU. We also compare the performance of running different models architectures such as convolutional and recurrent neural networks on CPU only vs using heterogeneous computing resources. Our result shows that although GPUs on the phones are capable of offering substantial performance gain in matrix multiplication on mobile devices. Therefore, models that involve multiplication of large matrices can run much faster (approx. 3 times faster in our experiments) due to GPU support.},
  eventtitle = {The 1st {{International Workshop}}},
  booktitle = {Proceedings of the 1st {{International Workshop}} on {{Deep Learning}} for {{Mobile Systems}} and {{Applications}} - {{EMDL}} '17},
  publisher = {{ACM Press}},
  urldate = {2019-12-12},
  date = {2017},
  pages = {7-12},
  author = {Alzantot, Moustafa and Wang, Yingnan and Ren, Zhengshuang and Srivastava, Mani B.},
  file = {/home/corentin/Zotero/storage/929TI5NW/Alzantot et al. - 2017 - RSTensorFlow GPU Enabled TensorFlow for Deep Lear.pdf}
}

@inproceedings{AlzantotRSTensorFlowGPUEnabled2017a,
  title = {{{RSTensorFlow}}: {{GPU Enabled TensorFlow}} for {{Deep Learning}} on {{Commodity Android Devices}}},
  volume = {2017},
  doi = {10.1145/3089801.3089805},
  shorttitle = {{{RSTensorFlow}}},
  abstract = {Mobile devices have become an essential part of our daily lives. By virtue of both their increasing computing power and the recent progress made in AI, mobile devices evolved to act as intelligent assistants in many tasks rather than a mere way of making phone calls. However, popular and commonly used tools and frameworks for machine intelligence are still lacking the ability to make proper use of the available heterogeneous computing resources on mobile devices. In this paper, we study the benefits of utilizing the heterogeneous (CPU and GPU) computing resources available on commodity android devices while running deep learning models. We leveraged the heterogeneous computing framework RenderScript to accelerate the execution of deep learning models on commodity Android devices. Our system is implemented as an extension to the popular open source framework TensorFlow. By integrating our acceleration framework tightly into TensorFlow, machine learning engineers can now easily make benefit of the heterogeneous computing resources on mobile devices without the need of any extra tools. We evaluate our system on different android phones models to study the trade-offs of running different neural network operations on the GPU. We also compare the performance of running different models architectures such as convolutional and recurrent neural networks on CPU only vs using heterogeneous computing resources. Our result shows that although GPUs on the phones are capable of offering substantial performance gain in matrix multiplication on mobile devices. Therefore, models that involve multiplication of large matrices can run much faster (approx. 3 times faster in our experiments) due to GPU support.},
  date = {2017-06-23},
  pages = {7-12},
  author = {Alzantot, Moustafa and Wang, Yingnan and Ren, Zhengshuang and Srivastava, Mani},
  file = {/home/corentin/Zotero/storage/ABUB9T64/Alzantot et al. - 2017 - RSTensorFlow GPU Enabled TensorFlow for Deep Lear.pdf}
}


